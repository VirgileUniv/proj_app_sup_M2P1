{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_train = 'data/wine_train.csv'\n",
    "PATH_test = 'data/wine_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# 3 digits floating points prints\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "rd_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>wine_type</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.26</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.054</td>\n",
       "      <td>41.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.997</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.55</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.018</td>\n",
       "      <td>45.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.989</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.34</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.028</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.993</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.32</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.29</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.044</td>\n",
       "      <td>33.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.995</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.36</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.037</td>\n",
       "      <td>33.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.992</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.49</td>\n",
       "      <td>11.1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.2              0.16         0.26             7.1      0.054   \n",
       "1            7.3              0.22         0.31             2.3      0.018   \n",
       "2            8.9              0.13         0.49             1.0      0.028   \n",
       "3            6.0              0.17         0.29             9.7      0.044   \n",
       "4            7.5              0.19         0.34             2.6      0.037   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 41.0                 224.0    0.997  3.38       0.55   \n",
       "1                 45.0                  80.0    0.989  3.06       0.34   \n",
       "2                  6.0                  24.0    0.993  2.91       0.32   \n",
       "3                 33.0                  98.0    0.995  3.12       0.36   \n",
       "4                 33.0                 125.0    0.992  3.10       0.49   \n",
       "\n",
       "   alcohol  wine_type  target  \n",
       "0     10.1          0       5  \n",
       "1     12.9          0       7  \n",
       "2      9.9          0       5  \n",
       "3      9.2          0       6  \n",
       "4     11.1          0       7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(PATH_train)\n",
    "data.drop(columns=['wine_ID'], inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4547 entries, 0 to 4546\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4547 non-null   float64\n",
      " 1   volatile acidity      4547 non-null   float64\n",
      " 2   citric acid           4547 non-null   float64\n",
      " 3   residual sugar        4547 non-null   float64\n",
      " 4   chlorides             4547 non-null   float64\n",
      " 5   free sulfur dioxide   4547 non-null   float64\n",
      " 6   total sulfur dioxide  4547 non-null   float64\n",
      " 7   density               4547 non-null   float64\n",
      " 8   pH                    4547 non-null   float64\n",
      " 9   sulphates             4547 non-null   float64\n",
      " 10  alcohol               4547 non-null   float64\n",
      " 11  wine_type             4547 non-null   int64  \n",
      " 12  target                4547 non-null   int64  \n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 461.9 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and target variable\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rd_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features to have a mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classification Models\n",
    "Define the classification models to be used for comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary regression models from sklearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "all_models = []\n",
    "all_models_names = []\n",
    "all_params = []\n",
    "\n",
    "\n",
    "# Define the regression models and their hyperparameters\n",
    "\n",
    "#1\n",
    "model = LogisticRegression()\n",
    "params = {}\n",
    "all_models.append(model)\n",
    "all_models_names.append('LogisticRegression')\n",
    "all_params.append(params)\n",
    "\n",
    "\n",
    "#2\n",
    "model = RandomForestClassifier(random_state=rd_state)\n",
    "params = {\n",
    "    'n_estimators': [100, 500, 1000], \n",
    "    # 'max_depth': [None, 5, 10, 20],\n",
    "}\n",
    "all_models.append(model)\n",
    "all_models_names.append('RandomForest')\n",
    "all_params.append(params)\n",
    "\n",
    "\n",
    "#3\n",
    "model = ExtraTreesClassifier(random_state=rd_state)\n",
    "params = {\n",
    "    'n_estimators': [100, 500, 1000], \n",
    "    # 'max_depth': [None, 5, 10, 20],\n",
    "}\n",
    "all_models.append(model)\n",
    "all_models_names.append('ExtraTrees')\n",
    "all_params.append(params)\n",
    "\n",
    "#4\n",
    "model = SVC(degree=5)\n",
    "params = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'C': [0.01, 0.1, 1, 10], \n",
    "}\n",
    "all_models.append(model)\n",
    "all_models_names.append('SVM')\n",
    "all_params.append(params)\n",
    "\n",
    "#5\n",
    "model = KNeighborsClassifier()\n",
    "params = {\n",
    "    # 'n_neighbors': [3, 5, 7, 9, 11], \n",
    "    'n_neighbors': np.round(np.linspace(2, 20, 5), 0).astype(int), \n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "all_models.append(model)\n",
    "all_models_names.append('KNeighbors')\n",
    "all_params.append(params)\n",
    "\n",
    "\n",
    "#6\n",
    "model = MLPClassifier(max_iter=500, random_state=rd_state)\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(8, 16), (8, 16, 32), (16, 32, 16)], \n",
    "    # 'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'alpha': [0.0001, 0.01, 0.1, 1],\n",
    "}\n",
    "all_models.append(model)\n",
    "all_models_names.append('MLP')\n",
    "all_params.append(params)\n",
    "\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "def create_lists_for_subset(models_names):\n",
    "    models = []\n",
    "    params = []\n",
    "    for i, name in enumerate(all_models_names) :\n",
    "        if name in models_names:\n",
    "            models.append(all_models[i])\n",
    "            params.append(all_params[i])        \n",
    "\n",
    "\n",
    "    return models, params\n",
    "\n",
    "# models_names = ['RandomForest', 'ExtraTrees']#, 'KNeighbors']\n",
    "models_names = all_models_names\n",
    "\n",
    "models, params = create_lists_for_subset(models_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with GridSearchCV\n",
    "Use GridSearchCV to tune the hyperparameters of each model and find the best performing set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>best_CVscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'n_estimators': 500}</td>\n",
       "      <td>0.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>{'n_estimators': 500}</td>\n",
       "      <td>0.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>{'C': 1, 'kernel': 'rbf'}</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>{'n_neighbors': 20, 'weights': 'distance'}</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>{'alpha': 1, 'hidden_layer_sizes': (16, 32, 16)}</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model                                   Best Parameters  \\\n",
       "0  LogisticRegression                                                {}   \n",
       "1        RandomForest                             {'n_estimators': 500}   \n",
       "2          ExtraTrees                             {'n_estimators': 500}   \n",
       "3                 SVM                         {'C': 1, 'kernel': 'rbf'}   \n",
       "4          KNeighbors        {'n_neighbors': 20, 'weights': 'distance'}   \n",
       "5                 MLP  {'alpha': 1, 'hidden_layer_sizes': (16, 32, 16)}   \n",
       "\n",
       "   best_CVscore  \n",
       "0         0.178  \n",
       "1         0.381  \n",
       "2         0.365  \n",
       "3         0.241  \n",
       "4         0.325  \n",
       "5         0.253  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create empty lists to store the best parameters and best scores for each model\n",
    "dfs = []\n",
    "best_params = []\n",
    "best_scores = []\n",
    "\n",
    "# Loop through each model and its corresponding hyperparameters\n",
    "for model, param in zip(models, params):\n",
    "    print(model.__class__.__name__)\n",
    "    \n",
    "    # Use GridSearchCV to find the best hyperparameters\n",
    "    grid_search = GridSearchCV(clone(model), param, cv=5, scoring='r2')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    # Get the best parameters and best score\n",
    "    dfs.append(pd.DataFrame(grid_search.cv_results_))\n",
    "    best_params.append(grid_search.best_params_)\n",
    "    best_scores.append(grid_search.best_score_)\n",
    "\n",
    "# Create a dataframe to store the best parameters and best scores for each model\n",
    "df_grid_search_score = pd.DataFrame({'Model': models_names, 'Best Parameters': best_params, 'best_CVscore': best_scores})\n",
    "\n",
    "# Display the best parameters and best scores for each model\n",
    "df_grid_search_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate each best models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MEEEE\\Documents\\University\\M2\\app non superevise\\project_clean\\env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Train Score  test_score\n",
       "0  LogisticRegression        0.194       0.190\n",
       "1        RandomForest        1.000       0.378\n",
       "2          ExtraTrees        1.000       0.355\n",
       "3                 SVM        0.311       0.238\n",
       "4          KNeighbors        1.000       0.265\n",
       "5                 MLP        0.317       0.244"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the model performance on the test set (using the best hyperparameters)\n",
    "\n",
    "# Create empty lists to store the R-squared scores for each model\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Loop through each model and its corresponding best parameters\n",
    "for model, param in zip(models, best_params):\n",
    "    # Fit the model on the training data using the best parameters\n",
    "    model.set_params(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the training and test data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the performance using the R-squared score\n",
    "    train_score = r2_score(y_train, y_train_pred)\n",
    "    test_score = r2_score(y_test, y_test_pred)\n",
    "    test_score_classif = r2_score(y_test, np.round(y_test_pred, 0))\n",
    "    \n",
    "    # Append the R-squared scores to the corresponding lists\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "\n",
    "# Create a dataframe to store the R-squared scores for each model\n",
    "df_test_score = pd.DataFrame({'Model': models_names, 'Train Score': train_scores, 'test_score': test_scores})\n",
    "\n",
    "# Display the R-squared scores for each model\n",
    "df_grid_search_score['test_score'] = df_test_score['test_score']\n",
    "df_test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>best_CVscore</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_classif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  best_CVscore  test_score  test_score_classif\n",
       "0  LogisticRegression         0.178       0.190               0.190\n",
       "1        RandomForest         0.381       0.378               0.378\n",
       "2          ExtraTrees         0.365       0.355               0.355\n",
       "3                 SVM         0.241       0.238               0.238\n",
       "4          KNeighbors         0.325       0.265               0.265\n",
       "5                 MLP         0.253       0.244               0.244"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_search_score.drop(columns=['Best Parameters'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the best model on the whole training set and make predictions on the test set\n",
    "\n",
    "using best model for the grid scores ?\n",
    "or the test on the test dataset left out from the real training dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the best model for the grid search score\n",
    "i_model = df_grid_search_score['best_CVscore'].idxmax()\n",
    "\n",
    "# for the best model on the \"fake\" test dataset\n",
    "i_model = df_test_score['test_score'].idxmax()\n",
    "\n",
    "model = clone(models[i_model])\n",
    "name_model = df_grid_search_score['Model'][i_model].replace(' ', '').replace('_', '')\n",
    "param_model = df_grid_search_score['Best Parameters'][i_model]\n",
    "\n",
    "\n",
    "# load the test dataset and make prediction using the best model \n",
    "test_data = pd.read_csv(PATH_test)\n",
    "train_data = pd.read_csv(PATH_train)\n",
    "\n",
    "X_submit_train, y_submit_train = train_data.drop(columns=['wine_ID', 'target']), train_data['target']\n",
    "\n",
    "X_submit, wine_ids = test_data.drop(columns=['wine_ID']), test_data['wine_ID']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_submit_train, X_submit = scaler.fit_transform(X_submit_train), scaler.transform(X_submit)\n",
    "\n",
    "\n",
    "# Predict on the test data using random forest\n",
    "model.fit(X_submit_train, y_submit_train)\n",
    "y_submit = model.predict(X_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to file\n",
    "param_model_str = ''\n",
    "for key, value in param_model.items():\n",
    "    param_model_str += key + '_' + str(value) + '_'\n",
    "param_model_str = param_model_str[:-1]\n",
    "\n",
    "\n",
    "filename = f'submits/wine_submit_class_{name_model}_{param_model_str}.csv'\n",
    "\n",
    "pd.DataFrame({'wine_ID':wine_ids, 'target':y_submit}).to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
